# Prefect Deployment Configuration
# PestRoutes to Snowflake Data Pipeline

name: pestroutes-pipeline
prefect-version: 3.4.5

# Build configuration
build:
  - prefect_docker.deployments.steps.build_docker_image:
      id: build-image
      requires: prefect-docker>=0.3.0
      image_name: pestroutes-pipeline
      tag: latest
      dockerfile: auto
      platform: "linux/amd64"

# Deployment definitions
deployments:
  # Full pipeline deployment - processes all entities for all offices
  - name: pestroutes-full-pipeline
    version: 1.0.0
    tags:
      - production
      - pestroutes
      - snowflake
      - etl
    description: |
      Complete PestRoutes to Snowflake data pipeline processing all 27 entities 
      across 17 offices with RAW and STAGING layer transformations.
    
    entrypoint: flows/main_pipeline.py:pestroutes_full_pipeline
    
    parameters:
      # Date range for data extraction (defaults to yesterday Pacific Time)
      # PestRoutes API expects dates in Pacific Time
      start_date: "{{ (now('America/Los_Angeles') - timedelta(days=1)).strftime('%Y-%m-%d') }}"
      end_date: "{{ now('America/Los_Angeles').strftime('%Y-%m-%d') }}"
      
      # Processing options
      batch_size: 5000
      max_retries: 3
      run_staging: true
      
      # Entity filtering (empty = all entities)
      entities_to_process: []
      
      # Office filtering (empty = all offices)
      offices_to_process: []

    work_pool:
      name: default-work-pool
      
    schedule:
      # Run daily at 2 AM UTC
      cron: "0 2 * * *"
      timezone: "America/Denver"

  # Incremental pipeline deployment - processes only recent data
  - name: pestroutes-incremental
    version: 1.0.0
    tags:
      - incremental
      - pestroutes
      - hourly
    description: |
      Incremental PestRoutes pipeline for processing recent updates.
      Runs every 4 hours to capture near real-time changes.
    
    entrypoint: flows/main_pipeline.py:pestroutes_incremental_pipeline
    
    parameters:
      # Look back 6 hours to ensure no missed data
      hours_lookback: 6
      batch_size: 1000
      max_retries: 2
      run_staging: true

    work_pool:
      name: default-work-pool
      
    schedule:
      # Run every 4 hours
      cron: "0 */4 * * *"
      timezone: "America/Denver"

  # Single entity deployment - for testing or specific entity runs
  - name: pestroutes-single-entity
    version: 1.0.0
    tags:
      - testing
      - single-entity
    description: |
      Single entity pipeline for testing or processing specific entities.
      Useful for development, testing, or ad-hoc entity processing.
    
    entrypoint: flows/main_pipeline.py:pestroutes_single_entity_pipeline
    
    parameters:
      entity: "customer"
      start_date: "{{ (now('America/Los_Angeles') - timedelta(days=1)).strftime('%Y-%m-%d') }}"
      end_date: "{{ now('America/Los_Angeles').strftime('%Y-%m-%d') }}"
      office_ids: []  # Empty = all offices
      run_staging: true

    work_pool:
      name: default-work-pool

# Pull configuration for retrieving code
pull:
  - prefect.deployments.steps.git_clone_project:
      id: clone-repo
      repository: "{{ prefect.blocks.github-repo.repository }}"
      credentials: "{{ prefect.blocks.github-evanunick }}"
      branch: main
      include_submodules: false

# Push configuration for storing flow code
push:
  - prefect_docker.deployments.steps.push_docker_image:
      requires: prefect-docker>=0.3.0
      image_name: "{{ build-image.image_name }}"
      tag: "{{ build-image.tag }}"